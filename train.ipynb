{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, Conv1D, GlobalMaxPooling1D, LSTM, Bidirectional,\n",
    "    Attention, Dense, Dropout, Concatenate\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import class_weight\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(filepath):\n",
    "    data = pd.read_json(filepath, lines=True)\n",
    "    sentences = data[\"headline\"].values\n",
    "    labels = data[\"is_sarcastic\"].values\n",
    "    return sentences, labels\n",
    "\n",
    "# Preprocess the dataset\n",
    "def preprocess(sentences, labels, max_words=10000, max_len=30):\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\")\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "    \n",
    "    # Save the tokenizer for later use\n",
    "    with open(\"tokenizer.pkl\", \"wb\") as file:\n",
    "        pickle.dump(tokenizer, file)\n",
    "    \n",
    "    return padded, encoded_labels, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec embeddings\n",
    "def load_pretrained_embeddings(word_index, embedding_dim=300, embedding_file=\"GoogleNews-vectors-negative300.bin\"):\n",
    "    word2vec = KeyedVectors.load_word2vec_format(embedding_file, binary=True, limit=500000)\n",
    "    vocab_size = len(word_index) + 1\n",
    "    embedding_matrix = np.random.uniform(-0.05, 0.05, (vocab_size, embedding_dim))  # Random initialization\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if word in word2vec:\n",
    "            embedding_matrix[i] = word2vec[word]\n",
    "    return embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "def build_model_with_embeddings(vocab_size, max_len, embedding_matrix, embedding_dim, filters=128, filter_width=3,\n",
    "                                 hidden_units=64, dropout_fraction=0.5):\n",
    "    input_layer = Input(shape=(max_len,))\n",
    "    embedding_layer = Embedding(input_dim=vocab_size, \n",
    "                                output_dim=embedding_dim,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=True)(input_layer)\n",
    "\n",
    "    # CNN Module\n",
    "    cnn_layer = Conv1D(filters=filters, kernel_size=filter_width, activation=\"relu\")(embedding_layer)\n",
    "    cnn_pooling = GlobalMaxPooling1D()(cnn_layer)\n",
    "\n",
    "    # BiLSTM Module\n",
    "    lstm_layer = Bidirectional(LSTM(hidden_units, return_sequences=True))(embedding_layer)\n",
    "    lstm_attention = Attention()([lstm_layer, lstm_layer])\n",
    "    lstm_output = GlobalMaxPooling1D()(lstm_attention)\n",
    "\n",
    "    # Combine CNN and LSTM outputs\n",
    "    combined = Concatenate()([cnn_pooling, lstm_output])\n",
    "\n",
    "    # Fully connected layers\n",
    "    dense_layer = Dense(hidden_units, activation=\"relu\")(combined)\n",
    "    dropout_layer = Dropout(dropout_fraction)(dense_layer)\n",
    "    output_layer = Dense(1, activation=\"sigmoid\")(dropout_layer)\n",
    "\n",
    "    # Build model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(optimizer=Adadelta(learning_rate=1.0), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with grid search\n",
    "def tune_hyperparameters(X_train, y_train, X_val, y_val, vocab_size, max_len, embedding_matrix, embedding_dim):\n",
    "    param_grid = {\n",
    "        'filters': [64, 128],\n",
    "        'filter_width': [3, 5],\n",
    "        'hidden_units': [64, 128],\n",
    "        'dropout_fraction': [0.3, 0.5]\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "    for filters in param_grid['filters']:\n",
    "        for filter_width in param_grid['filter_width']:\n",
    "            for hidden_units in param_grid['hidden_units']:\n",
    "                for dropout_fraction in param_grid['dropout_fraction']:\n",
    "                    print(f\"Testing with filters={filters}, filter_width={filter_width}, hidden_units={hidden_units}, dropout_fraction={dropout_fraction}\")\n",
    "                    model = build_model_with_embeddings(vocab_size, max_len, embedding_matrix, embedding_dim, \n",
    "                                                        filters=filters, filter_width=filter_width, \n",
    "                                                        hidden_units=hidden_units, dropout_fraction=dropout_fraction)\n",
    "                    history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                                        epochs=5, batch_size=32, verbose=0)\n",
    "                    val_accuracy = max(history.history['val_accuracy'])\n",
    "                    if val_accuracy > best_accuracy:\n",
    "                        best_accuracy = val_accuracy\n",
    "                        best_model = model\n",
    "\n",
    "    print(f\"Best validation accuracy: {best_accuracy:.4f}\")\n",
    "    return best_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "unexpected end of input; is count incorrect or file otherwise damaged?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load pre-trained embeddings\u001b[39;00m\n\u001b[0;32m     11\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[1;32m---> 12\u001b[0m embedding_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrained_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Split the data into train, validation, and test sets (80:10:10)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m X_train, X_temp, y_train, y_temp \u001b[38;5;241m=\u001b[39m train_test_split(padded, encoded_labels, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mload_pretrained_embeddings\u001b[1;34m(word_index, embedding_dim, embedding_file)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_pretrained_embeddings\u001b[39m(word_index, embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, embedding_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoogleNews-vectors-negative300.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     word2vec \u001b[38;5;241m=\u001b[39m \u001b[43mKeyedVectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      5\u001b[0m     embedding_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, (vocab_size, embedding_dim))  \u001b[38;5;66;03m# Random initialization\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BH BHAURYAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1672\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_word2vec_format\u001b[39m(\n\u001b[0;32m   1674\u001b[0m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m, unicode_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1675\u001b[0m         limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, datatype\u001b[38;5;241m=\u001b[39mREAL, no_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1676\u001b[0m     ):\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \n\u001b[0;32m   1679\u001b[0m \u001b[38;5;124;03m    Warnings\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1717\u001b[0m \n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1722\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\BH BHAURYAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:2065\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   2062\u001b[0m kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(vector_size, vocab_size, dtype\u001b[38;5;241m=\u001b[39mdatatype)\n\u001b[0;32m   2064\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m-> 2065\u001b[0m     \u001b[43m_word2vec_read_binary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2066\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcounts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary_chunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\n\u001b[0;32m   2067\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2069\u001b[0m     _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n",
      "File \u001b[1;32mc:\\Users\\BH BHAURYAL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gensim\\models\\keyedvectors.py:1966\u001b[0m, in \u001b[0;36m_word2vec_read_binary\u001b[1;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding)\u001b[0m\n\u001b[0;32m   1964\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tot_processed_words \u001b[38;5;241m!=\u001b[39m vocab_size:\n\u001b[1;32m-> 1966\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munexpected end of input; is count incorrect or file otherwise damaged?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mEOFError\u001b[0m: unexpected end of input; is count incorrect or file otherwise damaged?"
     ]
    }
   ],
   "source": [
    "# Main Workflow\n",
    "filepath = \"Sarcasm_Headlines_Dataset.json\"\n",
    "sentences, labels = load_data(filepath)\n",
    "\n",
    "# Preprocess the data\n",
    "max_len = 30\n",
    "max_words = 10000\n",
    "padded, encoded_labels, tokenizer = preprocess(sentences, labels, max_words=max_words, max_len=max_len)\n",
    "\n",
    "# Load pre-trained embeddings\n",
    "embedding_dim = 300\n",
    "embedding_matrix = load_pretrained_embeddings(tokenizer.word_index, embedding_dim)\n",
    "\n",
    "# Split the data into train, validation, and test sets (80:10:10)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(padded, encoded_labels, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Perform grid search to find the best model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "best_model = tune_hyperparameters(X_train, y_train, X_val, y_val, vocab_size, max_len, embedding_matrix, embedding_dim)\n",
    "\n",
    "# Train the best model with early stopping\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32, class_weight=class_weights_dict, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}, Test Loss: {test_loss:.2f}\")\n",
    "\n",
    "# Save the final model\n",
    "best_model.save(\"sarcasm_model_with_tuning.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
